---
title: 'Advanced Machine Learning \linebreak LAB 3: Reinforcement Learning'
author: "Mohammed Bakheet (mohba508)"
date: "07/10/2020"
output:
  pdf_document: default
  html_document:
    df_print: paged
toc: yes
---

```{r setup, include=FALSE}
RNGversion('3.5.1')
knitr::opts_chunk$set(echo = TRUE)
library(HMM)
library(entropy)
set.seed(12345)
```

\newpage  

# Question 1: Q-Learning

The file RL Lab1.R in the course website contains a template of the Q- learning algorithm. 1 You are asked to complete the implementation. We will work with a grid-world environment consisting of H × W tiles laid out in a 2-dimensional grid. An agent acts by moving up, down, left or right in the grid-world. This corresponds to the following Markov decision process:  

- State space: S = {(x, y) | x $\in$  {1, . . . , H}, y $\in$ {1, . . . , W }}.  
- Action space: A = {up, down, left, right}.  

Additionally, we assume state space to be fully observable. The reward function is a deterministic function of the state and does not depend on the actions taken by the agent. We assume the agent gets the reward as soon as it moves to a state. The transition model is defined by the agent moving in the direction chosen with probability (1 - $\beta$). The agent might also slip and end up moving in the direction to the left or right of its chosen action, each with probability $\beta$/2.
The transition model is unknown to the agent, forcing us to resort to model-free solutions. The environment is episodic and all states with a non-zero reward are terminal. Throughout this lab we use integer representations of the different actions: Up=1, right=2, down=3 and left=4.  


```{r question1, echo=F, dev='cairo_pdf'}
library(ggplot2)

# If you do not see four arrows in line 16, then do the following:
# File/Reopen with Encoding/UTF-8

arrows <- c("↑", "→", "↓", "←")
action_deltas <- list(c(1,0), # up
                      c(0,1), # right
                      c(-1,0), # down
                      c(0,-1)) # left

vis_environment <- function(iterations=0, epsilon = 0.5, alpha = 0.1, gamma = 0.95, beta = 0){
  
  # Visualize an environment with rewards. 
  # Q-values for all actions are displayed on the edges of each tile.
  # The (greedy) policy for each state is also displayed.
  # 
  # Args:
  #   iterations, epsilon, alpha, gamma, beta (optional): for the figure title.
  #   reward_map (global variable): a HxW array containing the reward given at each state.
  #   q_table (global variable): a HxWx4 array containing Q-values for each state-action pair.
  #   H, W (global variables): environment dimensions.
  
  df <- expand.grid(x=1:H,y=1:W)
  foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,q_table[x,y,1],NA),df$x,df$y)
  df$val1 <- as.vector(round(foo, 2))
  foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,q_table[x,y,2],NA),df$x,df$y)
  df$val2 <- as.vector(round(foo, 2))
  foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,q_table[x,y,3],NA),df$x,df$y)
  df$val3 <- as.vector(round(foo, 2))
  foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,q_table[x,y,4],NA),df$x,df$y)
  df$val4 <- as.vector(round(foo, 2))
  foo <- mapply(function(x,y) 
    ifelse(reward_map[x,y] == 0,arrows[GreedyPolicy(x,y)],reward_map[x,y]),df$x,df$y)
  df$val5 <- as.vector(foo)
  foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,max(q_table[x,y,]),
                                     ifelse(reward_map[x,y]<0,NA,reward_map[x,y])),df$x,df$y)
  df$val6 <- as.vector(foo)
  
  print(ggplot(df,aes(x = y,y = x)) +
          scale_fill_gradient(low = "white", high = "green", na.value = "red", name = "") +
          geom_tile(aes(fill=val6)) +
          geom_text(aes(label = val1),size = 4,nudge_y = .35,na.rm = TRUE) +
          geom_text(aes(label = val2),size = 4,nudge_x = .35,na.rm = TRUE) +
          geom_text(aes(label = val3),size = 4,nudge_y = -.35,na.rm = TRUE) +
          geom_text(aes(label = val4),size = 4,nudge_x = -.35,na.rm = TRUE) +
          geom_text(aes(label = val5),size = 10) +
          geom_tile(fill = 'transparent', colour = 'black') + 
          ggtitle(paste("Q-table after ",iterations," iterations\n",
                        "(epsilon = ",epsilon,", alpha = ",alpha,"gamma = ",gamma,", beta = ",beta,")")) +
          theme(plot.title = element_text(hjust = 0.5)) +
          scale_x_continuous(breaks = c(1:W),labels = c(1:W)) +
          scale_y_continuous(breaks = c(1:H),labels = c(1:H)))
  
}


```

## Greedy Policy Function  

```{r greedy_policy}
GreedyPolicy <- function(x, y){
  
  # Get a greedy action for state (x,y) from q_table.
  #
  # Args:
  #   x, y: state coordinates.
  #   q_table (global variable): a HxWx4 array containing Q-values for each state-action pair.
  # 
  # Returns:
  #   An action, i.e. integer in {1,2,3,4}.
  
  # Your code here.
  rewards = q_table[x,y,1:4]
  action = 0
  max_reward = which(rewards == max(rewards))
  
  if (length(max_reward)>1){
    action = sample(max_reward,1)
  }
  else
  {
    action = max_reward
  }
  return(action)
}


```


## Epsilon Greedy Function  

```{r epsilongreedy}
EpsilonGreedyPolicy <- function(x, y, epsilon){
  
  # Get an epsilon-greedy action for state (x,y) from q_table.
  #
  # Args:
  #   x, y: state coordinates.
  #   epsilon: probability of acting greedily.
  # 
  # Returns:
  #   An action, i.e. integer in {1,2,3,4}.
  
  # Your code here.
  actions = c(1:4)
  action = 0
  rewards = q_table[x,y,1:4]
  
  max_reward = which(rewards == max(rewards))
  
  if (length(max_reward)>1){
    action = sample(max_reward,1)
  }
  else
  {
    action = max_reward
  }
  
  if (1-epsilon < runif(1)){
    return(sample(actions,1))
  }
  else{
    return(action)
  }
}

```

## Transition Model  

```{r transition_model}
transition_model <- function(x, y, action, beta){
  
  # Computes the new state after given action is taken. The agent will follow the action 
  # with probability (1-beta) and slip to the right or left with probability beta/2 each.
  # 
  # Args:
  #   x, y: state coordinates.
  #   action: which action the agent takes (in {1,2,3,4}).
  #   beta: probability of the agent slipping to the side when trying to move.
  #   H, W (global variables): environment dimensions.
  # 
  # Returns:
  #   The new state after the action has been taken.
  
  delta <- sample(-1:1, size = 1, prob = c(0.5*beta,1-beta,0.5*beta))
  final_action <- ((action + delta + 3) %% 4) + 1
  foo <- c(x,y) + unlist(action_deltas[final_action])
  foo <- pmax(c(1,1),pmin(foo,c(H,W)))
  
  return (foo)
}
```

## Q Learning Function  

```{r q_learning, echo=F}
q_learning <- function(start_state, epsilon = 0.5, alpha = 0.1, gamma = 0.95, 
                       beta = 0){
  
  # Perform one episode of Q-learning. The agent should move around in the 
  # environment using the given transition model and update the Q-table.
  # The episode ends when the agent reaches a terminal state.
  # 
  # Args:
  #   start_state: array with two entries, describing the starting position of the agent.
  #   epsilon (optional): probability of acting greedily.
  #   alpha (optional): learning rate.
  #   gamma (optional): discount factor.
  #   beta (optional): slipping factor.
  #   reward_map (global variable): a HxW array containing the reward given at each state.
  #   q_table (global variable): a HxWx4 array containing Q-values for each state-action pair.
  # 
  # Returns:
  #   reward: reward received in the episode.
  #   correction: sum of the temporal difference correction terms over the episode.
  #   q_table (global variable): Recall that R passes arguments by value. So, q_table being
  #   a global variable can be modified with the superassigment operator <<-.
  
  # Your code here.
  reward = 0
  episode_correction = 0
  
  repeat{
    # Follow policy, execute action, get reward.
    
    #Taking an action
    old_x = start_state[1]
    old_y = start_state[2]

    action= EpsilonGreedyPolicy(old_x, old_y, epsilon)
    
    #Computing the new state after the action is taken
    new_state = transition_model(old_x, old_y, action, beta)
    new_x = new_state[1]
    new_y = new_state[2]
    
    #Computing the current reward
    current_reward = reward_map[new_x, new_y]
    
    # Q-table update.
    #Getting a greedy action
    maximum_action = GreedyPolicy(new_x,new_y)
    
    #Calculating the final reward
    reward = reward+current_reward
    
    #Calculating the episode_correction
    episode_correction = current_reward + gamma*q_table[new_x,new_y,maximum_action]-q_table[old_x,old_y,action]

    q_table[old_x,old_y,action] <<- q_table[old_x,old_y,action]+ alpha*episode_correction
    
    start_state = new_state
    if(reward!=0)
      # End episode.
      return (c(reward,epsilon_correction))
  }
  
}

```  

## Environment A:  

For our first environment, we will use H = 5 and W = 7. This environment includes a reward of 10 in state (3,6) and a reward of -1 in states (2,3), (3,3) and (4,3).  
We specify the rewards using a reward map in the form of a matrix with one entry for each state. States with no reward will simply have a matrix entry of 0. The agent starts each episode in the state (3,1). The function vis_environment in the file RL Lab1.R is used to visualize the environment and learned action values and policy. You will not have to modify this function, but read the comments in it to familiarize with how it can be used.  
When implementing Q-learning, the estimated values of Q(S, A) are commonly stored in a data-structured called Q-table. This is nothing but a tensor with one entry for each state-action pair. Since we have a H × W environment with four actions, we can use a 3D-tensor of dimensions H × W × 4 to represent our Q-table. Initialize all Q-values to 0. Run the function vis_environment before proceeding further. Note that each non-terminal tile has four values. These represent the action values associated to the tile (state). Note also that each non-terminal tile has an arrow. This indicates the greedy policy for the tile (ties are broken at random).  

Your are requested to carry out the following tasks:  
- Implement the greedy and $\epsilon$-greedy policies in the functions GreedyPolicy and EpsilonGreedyPolicy of the file RL Lab1.R. The functions should break ties at random, i.e. they should sample uniformly from the set of actions with maximal Q-value.  

- Implement the Q-learning algorithm in the function q learning of the file RL Lab1.R.  
The function should run one episode of the agent acting in the environment and update the Q-table accordingly. The function should return the episode reward and the sum of the temporal-difference correction terms $R + \gamma * max_a Q(S ` , a) - Q(S, A)$ for all steps in the episode. Note that a transition model taking $\beta$ as input is already implemented for you in the function transition model.  

- Run 10000 episodes of Q-learning with $\epsilon$ = 0.5, $\beta$ = 0, $\alpha$ = 0.1 and $\gamma$ = 0.95. To do so, simply run the code provided in the file RL Lab1.R. The code visualizes the Q-table and a greedy policy derived from it after episodes 10, 100, 1000 and 10000. Answer the following questions:  

– What has the agent learned after the first 10 episodes ?  
– Is the final greedy policy (after 10000 episodes) optimal? Why / Why not ?  
– Does the agent learn that there are multiple paths to get to the positive reward ?  
If not, what could be done to make the agent learn this ?  

### Answers:  

- After the first 10 episodes the agent hasn't learned much, it's got knowledge about avoiding the states with negative ones (-1).  
- After 10000 iterations, the agent has learned how to reach the goal following the optimal path, in some states i.e (1,3) the goodness of some actions was the same, but still the agent was able to pick the correct path to the target state. if we increase the number of iterations the accuracy will increase, but we are satisfied with the current policy and we can say the solution is optimal.

- The agent has learned how to reach to the goal from the state following one paths, it takes one path to the goal and it doesn't slip due the $\beta$ value of 0 that doesn't allow the agent to slip from the path. And if we want the agent to try different paths, then we can increase the value of $\beta$ that allows the agent to slip from the path and try different paths to the target.  



```{r environmentA, message=FALSE, warning=FALSE, dev='cairo_pdf'}

# Environment A (learning)

H <- 5
W <- 7

reward_map <- matrix(0, nrow = H, ncol = W)
reward_map[3,6] <- 10
reward_map[2:4,3] <- -1

q_table <- array(0,dim = c(H,W,4))

vis_environment()

for(i in 1:10000){
  foo <- q_learning(start_state = c(3,1))
  
  if(any(i==c(10,100,1000,10000)))
    vis_environment(i)
}

```


## Environment B:  

This is a 7×8 environment where the top and bottom rows have negative rewards. In this environment, the agent starts each episode in the state (4, 1). There are two positive rewards, of 5 and 10. The reward of 5 is easily reachable, but the agent has to navigate around the first reward in order to find the reward worth 10.  
Your task is to investigate how the $\epsilon$ and $\gamma$ parameters affect the learned policy by running 30000 episodes of Q-learning with $\epsilon$ = 0.1, 0.5, $\gamma$ = 0.5, 0.75, 0.95, $\beta$ = 0 and $\alpha$ = 0.1. To do so, simply run the code provided in the file RL Lab1.R and explain your observations.  

### Answer:  

As we can see from the plot when:  
($\epsilon$ = 0.1 , $\alpha$ = 0.1 $\gamma$ = 0.5 , $\beta$ = 0), vs  ($\epsilon$ = 0.1 , $\alpha$ = 0.1 $\gamma$ = 0.75 , $\beta$ = 0) vs ($\epsilon$ = 0.1 , $\alpha$ = 0.1 $\gamma$ = 0.95 , $\beta$ = 0)  
When we increased the value of $\gamma$ from 0.5 to 0.75 with the same value of $\epsilon$, the agent hasn't explored after reaching the state with 5 reward because we increased the discount factor that determines whether the agent should take the immediate reward or explore (considers rewards later in time). And when using an $\gamma$ value of 0.95, the agent has learned more about how to reach the state with 5 rewards (if we have the same preferences, then $\gamma$ should be 1), but still the agent has no idea about the state with 10 rewards because the value of $\epsilon$ is small which means not taking a greedy action exploring with the discount factor, and the number of iterations also effect the learning $\gamma$  

Nonetheless, when the value of $\epsilon$ increases to 5, then the agent learns about the state with 10 reward, and that is because a higher value of $\epsilon$ allows the agent to explore the environment more by not taking the action that maximizes the reward all the time, but with probability.  


```{r environmentB, message=FALSE, warning=FALSE, dev='cairo_pdf'}

# Environment B (the effect of epsilon and gamma)

H <- 7
W <- 8

reward_map <- matrix(0, nrow = H, ncol = W)
reward_map[1,] <- -1
reward_map[7,] <- -1
reward_map[4,5] <- 5
reward_map[4,8] <- 10

q_table <- array(0,dim = c(H,W,4))

vis_environment()

MovingAverage <- function(x, n){
  
  cx <- c(0,cumsum(x))
  rsum <- (cx[(n+1):length(cx)] - cx[1:(length(cx) - n)]) / n
  
  return (rsum)
}

for(j in c(0.5,0.75,0.95)){
  q_table <- array(0,dim = c(H,W,4))
  reward <- NULL
  correction <- NULL
  
  for(i in 1:30000){
    foo <- q_learning(gamma = j, start_state = c(4,1))
    reward <- c(reward,foo[1])
    correction <- c(correction,foo[2])
  }
  
  vis_environment(i, gamma = j)
  plot(MovingAverage(reward,100),type = "l")
  plot(MovingAverage(correction,100),type = "l")
}

for(j in c(0.5,0.75,0.95)){
  q_table <- array(0,dim = c(H,W,4))
  reward <- NULL
  correction <- NULL
  
  for(i in 1:30000){
    foo <- q_learning(epsilon = 0.1, gamma = j, start_state = c(4,1))
    reward <- c(reward,foo[1])
    correction <- c(correction,foo[2])
  }
  
  vis_environment(i, epsilon = 0.1, gamma = j)
  plot(MovingAverage(reward,100),type = "l")
  plot(MovingAverage(correction,100),type = "l")
}
```  

## Environment C  

This is a smaller 3 × 6 environment. Here the agent starts each episode in the state (1,1). Your task is to investigate how the $\beta$ parameter affects the learned policy by running 10000 episodes of Q-learning with $\beta$ = 0, 0.2, 0.4, 0.66, $\epsilon$ = 0.5, $\gamma$ = 0.6 and $\alpha$ = 0.1. To do so, simply run the code provided in the file RL Lab1.R and explain your observations.  

### Answer:  

As it's seen in the plots when we increase the value of $\beta$, the frequency of the agent slipping from the path to the goal increases, for the highest values of $\beta$ (0.66) it goes in a loop in some states.   

```{r environmentc, message=FALSE, warning=FALSE, dev='cairo_pdf'}

# Environment C (the effect of beta).

H <- 3
W <- 6

reward_map <- matrix(0, nrow = H, ncol = W)
reward_map[1,2:5] <- -1
reward_map[1,6] <- 10

q_table <- array(0,dim = c(H,W,4))

vis_environment()

for(j in c(0,0.2,0.4,0.66)){
  q_table <- array(0,dim = c(H,W,4))
  
  for(i in 1:10000)
    foo <- q_learning(gamma = 0.6, beta = j, start_state = c(1,1))
  
  vis_environment(i, gamma = 0.6, beta = j)
}
```


# Question 2: Reinforce:  

The file RL Lab2.R in the course website contains an implementation of the REINFORCE algorithm. 2 Your task is to run the code provided and answer some questions. Although you do not have to modify the code, you are advised to check it out to familiarize with it. The code uses the R package keras to manipulate neural networks.  

We will work with a 4 × 4 grid. We want the agent to learn to navigate to a random goal position in the grid. The agent will start in a random position and it will be told the goal position.  
The agent receives a reward of 5 when it reaches the goal. Since the goal position can be any position, we need a way to tell the agent where the goal is. Since our agent does not have any memory mechanism, we provide the goal coordinates as part of the state at every time step, i.e. a state consists now of four coordinates: Two for the position of the agent, and
two for the goal position. The actions of the agent can however only impact its own position, i.e. the actions do not modify the goal position. Note that the agent initially does not know that the last two coordinates of a state indicate the position with maximal reward, i.e. the goal position. It has to learn it. It also has to learn a policy to reach the goal position from the initial position. Moreover, the policy has to depend on the goal position, because it is chosen
at random in each episode. Since we only have a single non-zero reward, we do not specify a reward map. Instead, the goal coordinates are passed to the functions that need to access the reward function.  

## Visualizing the environment:  


```{r visualization, echo=F, message=FALSE, warning=FALSE, dev='cairo_pdf'}
library(keras)

# install.packages("ggplot2")
# install.packages("vctrs")
library(ggplot2)

# If you do not see four arrows in line 19, then do the following:
# File/Reopen with Encoding/UTF-8

arrows <- c("↑", "→", "↓", "←")
action_deltas <- list(c(1,0), # up
                      c(0,1), # right
                      c(-1,0), # down
                      c(0,-1)) # left

vis_prob <- function(goal, episodes = 0){
  
  # Visualize an environment with rewards. 
  # Probabilities for all actions are displayed on the edges of each tile.
  # The (greedy) policy for each state is also displayed.
  # 
  # Args:
  #   goal: goal coordinates, array with 2 entries.
  #   episodes, epsilon, alpha, gamma, beta (optional): for the figure title.
  #   H, W (global variables): environment dimensions.
  
  df <- expand.grid(x=1:H,y=1:W)
  dist <- array(data = NA, dim = c(H,W,4))
  class <- array(data = NA, dim = c(H,W))
  for(i in 1:H)
    for(j in 1:W){
      dist[i,j,] <- DeepPolicy_dist(i,j,goal[1],goal[2])
      foo <- which(dist[i,j,]==max(dist[i,j,]))
      class[i,j] <- ifelse(length(foo)>1,sample(foo, size = 1),foo)
    }
  
  foo <- mapply(function(x,y) ifelse(all(c(x,y) == goal),NA,dist[x,y,1]),df$x,df$y)
  df$val1 <- as.vector(round(foo, 2))
  foo <- mapply(function(x,y) ifelse(all(c(x,y) == goal),NA,dist[x,y,2]),df$x,df$y)
  df$val2 <- as.vector(round(foo, 2))
  foo <- mapply(function(x,y) ifelse(all(c(x,y) == goal),NA,dist[x,y,3]),df$x,df$y)
  df$val3 <- as.vector(round(foo, 2))
  foo <- mapply(function(x,y) ifelse(all(c(x,y) == goal),NA,dist[x,y,4]),df$x,df$y)
  df$val4 <- as.vector(round(foo, 2))
  foo <- mapply(function(x,y) ifelse(all(c(x,y) == goal),NA,class[x,y]),df$x,df$y)
  df$val5 <- as.vector(arrows[foo])
  foo <- mapply(function(x,y) ifelse(all(c(x,y) == goal),"Goal",NA),df$x,df$y)
  df$val6 <- as.vector(foo)
  
  print(ggplot(df,aes(x = y,y = x)) +
          geom_tile(fill = 'white', colour = 'black') +
          scale_fill_manual(values = c('green')) +
          geom_tile(aes(fill=val6), show.legend = FALSE, colour = 'black') +
          geom_text(aes(label = val1),size = 4,nudge_y = .35,na.rm = TRUE) +
          geom_text(aes(label = val2),size = 4,nudge_x = .35,na.rm = TRUE) +
          geom_text(aes(label = val3),size = 4,nudge_y = -.35,na.rm = TRUE) +
          geom_text(aes(label = val4),size = 4,nudge_x = -.35,na.rm = TRUE) +
          geom_text(aes(label = val5),size = 10,na.rm = TRUE) +
          geom_text(aes(label = val6),size = 10,na.rm = TRUE) +
          ggtitle(paste("Action probabilities after ",episodes," episodes")) +
          theme(plot.title = element_text(hjust = 0.5)) +
          scale_x_continuous(breaks = c(1:W),labels = c(1:W)) +
          scale_y_continuous(breaks = c(1:H),labels = c(1:H)))
  
}
```

## Transition Model:  

```{r reinforce_transition, message=FALSE, warning=FALSE}
transition_model <- function(x, y, action, beta){
  
  # Computes the new state after given action is taken. The agent will follow the action 
  # with probability (1-beta) and slip to the right or left with probability beta/2 each.
  # 
  # Args:
  #   x, y: state coordinates.
  #   action: which action the agent takes (in {1,2,3,4}).
  #   beta: probability of the agent slipping to the side when trying to move.
  #   H, W (global variables): environment dimensions.
  # 
  # Returns:
  #   The new state after the action has been taken.
  
  delta <- sample(-1:1, size = 1, prob = c(0.5*beta,1-beta,0.5*beta))
  final_action <- ((action + delta + 3) %% 4) + 1
  foo <- c(x,y) + unlist(action_deltas[final_action])
  foo <- pmax(c(1,1),pmin(foo,c(H,W)))
  
  return (foo)
}
```

## Deep Policy Distribution:  

```{r reinforce_deep_policy_dist, message=FALSE, warning=FALSE}
DeepPolicy_dist <- function(x, y, goal_x, goal_y){
  
  # Get distribution over actions for state (x,y) and goal (goal_x,goal_y) from the deep policy.
  #
  # Args:
  #   x, y: state coordinates.
  #   goal_x, goal_y: goal coordinates.
  #   model (global variable): NN encoding the policy.
  # 
  # Returns:
  #   A distribution over actions.
  
  foo <- matrix(data = c(x,y,goal_x,goal_y), nrow = 1)
  
  # return (predict_proba(model, x = foo))
  return (predict_on_batch(model, x = foo)) # Faster.
  
}
```

## Deep Policy  

```{r reinforce_deep_policy, message=FALSE, warning=FALSE}
DeepPolicy <- function(x, y, goal_x, goal_y){
  
  # Get an action for state (x,y) and goal (goal_x,goal_y) from the deep policy.
  #
  # Args:
  #   x, y: state coordinates.
  #   goal_x, goal_y: goal coordinates.
  #   model (global variable): NN encoding the policy.
  # 
  # Returns:
  #   An action, i.e. integer in {1,2,3,4}.
  
  foo <- DeepPolicy_dist(x,y,goal_x,goal_y)
  
  return (sample(1:4, size = 1, prob = foo))
  
}

```  


## Deep Policy Training  

```{r deep_policy_training, message=FALSE, warning=FALSE}
DeepPolicy_train <- function(states, actions, goal, gamma){
  
  # Train the policy network on a rolled out trajectory.
  # 
  # Args:
  #   states: array of states visited throughout the trajectory.
  #   actions: array of actions taken throughout the trajectory.
  #   goal: goal coordinates, array with 2 entries.
  #   gamma: discount factor.
  
  # Construct batch for training.
  inputs <- matrix(data = states, ncol = 2, byrow = TRUE)
  inputs <- cbind(inputs,rep(goal[1],nrow(inputs)))
  inputs <- cbind(inputs,rep(goal[2],nrow(inputs)))
  
  targets <- array(data = actions, dim = nrow(inputs))
  targets <- to_categorical(targets-1, num_classes = 4)  
  
  # Sample weights. Reward of 5 for reaching the goal.
  weights <- array(data = 5*(gamma^(nrow(inputs)-1)), dim = nrow(inputs))
  
  # Train on batch. Note that this runs a SINGLE gradient update.
  train_on_batch(model, x = inputs, y = targets, sample_weight = weights)
  
}

```  

## Reinforcement Episode  

```{r reinforce_episode, message=FALSE, warning=FALSE}
reinforce_episode <- function(goal, gamma = 0.95, beta = 0){
  
  # Rolls out a trajectory in the environment until the goal is reached.
  # Then trains the policy using the collected states, actions and rewards.
  # 
  # Args:
  #   goal: goal coordinates, array with 2 entries.
  #   gamma (optional): discount factor.
  #   beta (optional): probability of slipping in the transition model.
  
  # Randomize starting position.
  cur_pos <- goal
  while(all(cur_pos == goal))
    cur_pos <- c(sample(1:H, size = 1),sample(1:W, size = 1))
  
  states <- NULL
  actions <- NULL
  
  steps <- 0 # To avoid getting stuck and/or training on unnecessarily long episodes.
  while(steps < 20){
    steps <- steps+1
    
    # Follow policy and execute action.
    action <- DeepPolicy(cur_pos[1], cur_pos[2], goal[1], goal[2])
    new_pos <- transition_model(cur_pos[1], cur_pos[2], action, beta)
    
    # Store states and actions.
    states <- c(states,cur_pos)
    actions <- c(actions,action)
    cur_pos <- new_pos
    
    if(all(new_pos == goal)){
      # Train network.
      DeepPolicy_train(states,actions,goal,gamma)
      break
    }
  }
  
}
```  


## Environment D:  

2.6. Environment D. In this task, we will use eight goal positions for training and, then, validate the learned policy on the remaining eight possible goal positions. The training and validation goal positions are stored in the lists train goals and val goals in the code in the file RL Lab2.R. You are requested to run the code provided, which runs the REINFORCE
algorithm for 5000 episodes with $\beta$ = 0 and $\gamma$ = 0.95. 3 Each training episode uses a random goal position from train goals. The initial position for the episode is also chosen at random. When training is completed, the code validates the learned policy for the goal positions in val goals. This is done by with the help of the function vis prob, which shows the grid, goal position and learned policy. Note that each non-terminal tile has four values. These represent the action probabilities associated to the tile (state). Note also that each non-terminal tile has an arrow. This indicates the action with the largest probability for the tile (ties are broken at random). Finally, answer the following questions:  

- Has the agent learned a good policy? Why / Why not ?  
- Could you have used the Q-learning algorithm to solve this task ?  

### Answer:  

Given a training goals: train_goals <- (4,1), (4,3), (3,1), (3,4), (2,1), (2,2), (1,2), (1,3)  
and validation goals: val_goals <- (4,2), (4,4), (3,2), (3,3), (2,3), (2,4), (1,1), (1,4)  

The agent could find its way to the validation goals because it was trained on a training goals that are similar to the validation goals. The training goals gave the agent a good view of the environment from the first, second, third, and fourth rows, and that's why the agent was able to find its way to the validation goals with high probability. In other words, the agent was able to generalize the policy for similar states.  

We can't use Q-Learning because it doesn't have the feature of generalization, for instance, if we set a different goal than the one Q-Learning was trained to find, then Q-learning won't be able to reach that goal. Reinforce has the generalization advantage which enables it to determine the action for states that it has not visited before given a similar state. So, if Reinforce visited a state that's similar to the state in interest, then it will give a similarly accurate result for that state.  


```{r environmentD, message=FALSE, warning=FALSE, dev='cairo_pdf'}

# Environment D (training with random goal positions)

H <- 4
W <- 4

# Define the neural network (two hidden layers of 32 units each).
model <- keras_model_sequential()
model %>% 
  layer_dense(units = 32, input_shape = c(4), activation = 'relu') %>% 
  layer_dense(units = 32, activation = 'relu') %>% 
  layer_dense(units = 4, activation = 'softmax')

compile(model, loss = "categorical_crossentropy", optimizer = optimizer_sgd(lr=0.001))

initial_weights <- get_weights(model)

train_goals <- list(c(4,1), c(4,3), c(3,1), c(3,4), c(2,1), c(2,2), c(1,2), c(1,3))
val_goals <- list(c(4,2), c(4,4), c(3,2), c(3,3), c(2,3), c(2,4), c(1,1), c(1,4))

show_validation <- function(episodes){
  
  for(goal in val_goals)
    vis_prob(goal, episodes)
  
}

set_weights(model,initial_weights)

show_validation(0)

for(i in 1:5000){
  if(i%%10==0) cat("episode",i,"\n")
  goal <- sample(train_goals, size = 1)
  reinforce_episode(unlist(goal))
}

show_validation(5000) 
```

## Environment E:  

2.7. Environment E. You are now asked to repeat the previous experiments but this time the goals for training are all from the top row of the grid. The validation goals are three positions from the rows below. To solve this task, simply run the code provided in the file RL Lab2.R and answer the following questions:  

- Has the agent learned a good policy? Why / Why not ?  
- If the results obtained for environments D and E differ, explain why. 

### Answer:  

The agent couldn't find its way to the validation goals from most of the states because the training was done on the first row and the agent is unable to generalize over a very different goals from the ones it was trained on. The closer the goal to the first row, the more accurately the generalization works. Even though, the agent was able to reach the goal from some of the states, but in most cases (the majority of states), it was unable to find its way to the validation goal.  

The result of environment E is different from that of D because the training goals are different and the validation goals are different. In D, the agent was trained on random goals from the environment with good exploration of different states, however, in E the agent was trained to reach goals on the first row only, which has made it difficult for it to reach validation goals that are far from the first row (unsimilar to the training goals).  



```{r environmentE, message=FALSE, warning=FALSE, dev='cairo_pdf'}

# Environment E (training with top row goal positions)

train_goals <- list(c(4,1), c(4,2), c(4,3), c(4,4))
val_goals <- list(c(3,4), c(2,3), c(1,1))

set_weights(model,initial_weights)

show_validation(0)

for(i in 1:5000){
  if(i%%10==0) cat("episode", i,"\n")
  goal <- sample(train_goals, size = 1)
  reinforce_episode(unlist(goal))
}

show_validation(5000) 
```


# References  
Course Documents  
https://stackoverflow.com/  


# Appendix

```{r ref.label=knitr::all_labels(), eval = FALSE}
```